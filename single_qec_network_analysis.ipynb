{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05252c11-3a9b-48e3-9824-09414f813c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading experiment data from: results_network_qec_v2 (Load full data: False)\n",
      "Finished loading. Loaded 636 experiment configurations.\n",
      "\n",
      "Total number of loaded experiment configurations: 636\n",
      "\n",
      "Example loaded data point:\n",
      "  State: -\n",
      "  QEC: none\n",
      "  Dist: 10.0\n",
      "  Error Type: bit_flip\n",
      "  Error Params: {'probability': 0.01}\n",
      "  Data points: (Full data not loaded)\n",
      "  Avg Fidelity: 0.9933\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import AutoMinorLocator, MultipleLocator\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "# Make sure this points to the correct directory containing the error_type subfolders\n",
    "RESULTS_DIR = \"results_network_qec_v2\"\n",
    "PLOTS_DIR = \"plots_network_qec_v2\"\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# --- Plotting Configuration (from original notebook) ---\n",
    "def configure_plots():\n",
    "    \"\"\"Configure matplotlib rcParams for publication-quality plots\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid') # Use an available style\n",
    "    mpl.rcParams.update({\n",
    "        'font.family': 'serif',\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'legend.fontsize': 11,\n",
    "        'legend.frameon': True,\n",
    "        'legend.framealpha': 0.8,\n",
    "        'figure.figsize': (11, 7), # Adjusted figure size\n",
    "        'figure.dpi': 300,\n",
    "        'text.usetex': False,  # Keep False for broader compatibility\n",
    "        'mathtext.fontset': 'stix', # Or 'cm' if text.usetex is True\n",
    "        'axes.prop_cycle': plt.cycler('color', plt.cm.viridis(np.linspace(0, 0.85, 8))), # Sample 8 colors from viridis\n",
    "        'axes.axisbelow': True,\n",
    "        'grid.alpha': 0.6,\n",
    "        'grid.linestyle': ':'\n",
    "    })\n",
    "\n",
    "configure_plots() # Apply the configuration\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib') # Ignore minor warnings\n",
    "\n",
    "# --- Data Loading Function ---\n",
    "def parse_param_string(param_str):\n",
    "    \"\"\"Parses parameter strings like 'gamma_0.1' or 'p_loss_init_0.0_p_loss_length_0.16'\"\"\"\n",
    "    params = {}\n",
    "    parts = param_str.split('_')\n",
    "    if not parts:\n",
    "        return params\n",
    "    i = 0\n",
    "    while i < len(parts):\n",
    "        # Combine parts until the next part looks like a number (or is the last part)\n",
    "        key = parts[i]\n",
    "        j = i + 1\n",
    "        # Look ahead: check if the next part starts a new key=value pair\n",
    "        # This is tricky, assume value is the last part or the part before the next non-numeric key\n",
    "        while j < len(parts) - 1:\n",
    "            # If the part after the potential value (parts[j+1]) looks like a key (alphabetic start)\n",
    "            # and parts[j] looks like a value (numeric start, or contains '.', or is '-'), stop combining.\n",
    "            is_numeric_val = parts[j][0].isdigit() or '.' in parts[j] or parts[j].startswith('-')\n",
    "            next_is_key = j+1 < len(parts) and parts[j+1] and parts[j+1][0].isalpha()\n",
    "\n",
    "            if is_numeric_val and next_is_key:\n",
    "                 break # parts[j] is likely the value for the current key\n",
    "\n",
    "            # Otherwise, combine this part into the key\n",
    "            key += \"_\" + parts[j]\n",
    "            j += 1\n",
    "\n",
    "        value_str = parts[j]\n",
    "        try:\n",
    "             value = float(value_str)\n",
    "        except ValueError:\n",
    "            value = value_str # Keep as string if not float\n",
    "\n",
    "        params[key] = value\n",
    "        i = j + 1 # Move to the start of the next potential key\n",
    "    return params\n",
    "\n",
    "def load_network_qec_data(results_dir=RESULTS_DIR, load_full_data=False):\n",
    "    \"\"\"\n",
    "    Loads experiment data from the network_qec_simulation results structure.\n",
    "    Handles potentially malformed headers and assumes data rows have iteration, fidelity.\n",
    "\n",
    "    Args:\n",
    "        results_dir (str): The path to the main results directory.\n",
    "        load_full_data (bool): If True, loads the full DataFrame for each file.\n",
    "                               Otherwise, only loads metrics. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents one\n",
    "              experiment run and contains its parameters and loaded data/metrics.\n",
    "    \"\"\"\n",
    "    print(f\"Loading experiment data from: {results_dir} (Load full data: {load_full_data})\")\n",
    "    all_experiments_data = []\n",
    "    skipped_files = []\n",
    "    loaded_count = 0\n",
    "    file_parse_errors = []\n",
    "\n",
    "    filename_pattern = re.compile(r\"^([+\\-01])_(.+?)_d([\\d\\.]+?)_(.+)\\.csv$\")\n",
    "    error_type_dirs = [d for d in os.listdir(results_dir) if os.path.isdir(os.path.join(results_dir, d))]\n",
    "\n",
    "    for error_type in error_type_dirs:\n",
    "        error_dir_path = os.path.join(results_dir, error_type)\n",
    "        # print(f\" Processing directory: {error_dir_path}\") # Reduce verbosity\n",
    "\n",
    "        for filename in os.listdir(error_dir_path):\n",
    "            if not filename.endswith(\".csv\") or filename.endswith(\"_metadata.csv\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(error_dir_path, filename)\n",
    "\n",
    "            # --- Check 1: File Size --- #\n",
    "            try:\n",
    "                if os.path.getsize(file_path) == 0:\n",
    "                    # print(f\"  - Skipping 0-byte file: {filename}\")\n",
    "                    skipped_files.append(filename)\n",
    "                    continue\n",
    "            except OSError as size_e:\n",
    "                # print(f\"  - Error checking size for {filename}: {size_e}\")\n",
    "                file_parse_errors.append((filename, f\"Size check error: {size_e}\"))\n",
    "                skipped_files.append(filename)\n",
    "                continue\n",
    "            # --- End Check 1 --- #\n",
    "\n",
    "            match = filename_pattern.match(filename)\n",
    "            if match:\n",
    "                initial_state = match.group(1)\n",
    "                qec_method = match.group(2)\n",
    "                distance = float(match.group(3))\n",
    "                param_string = match.group(4)\n",
    "\n",
    "                try:\n",
    "                    error_params = parse_param_string(param_string)\n",
    "                    if not error_params:\n",
    "                        # print(f\"  - Warning: Could not parse parameters from '{param_string}' in file {filename}\")\n",
    "                        skipped_files.append(filename)\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        # --- Modified pd.read_csv (Attempt 2) --- #\n",
    "                        data_df = pd.read_csv(\n",
    "                            file_path,\n",
    "                            header=None,         # Still ignore the actual header row\n",
    "                            skiprows=1,          # Still skip the first row\n",
    "                            names=['iteration', 'fidelity'], # Expect exactly two columns\n",
    "                            # usecols removed as we expect exactly the named columns\n",
    "                            on_bad_lines='warn'\n",
    "                        )\n",
    "                        # --- End of modification --- #\n",
    "\n",
    "                        # --- Check 2: Empty DataFrame after load? --- #\n",
    "                        if data_df.empty:\n",
    "                            # print(f\"  - Warning: DataFrame empty after loading (check content/header): {filename}\")\n",
    "                            skipped_files.append(filename)\n",
    "                            continue\n",
    "                        # --- End Check 2 --- #\n",
    "\n",
    "                        # --- Check 3: Drop NaNs and check again --- #\n",
    "                        # Ensure fidelity column exists before trying to coerce/dropna\n",
    "                        if 'fidelity' not in data_df.columns:\n",
    "                            # print(f\"  - Warning: 'fidelity' column not found after loading: {filename}\")\n",
    "                            skipped_files.append(filename)\n",
    "                            continue\n",
    "\n",
    "                        # Convert fidelity to numeric, coercing errors\n",
    "                        data_df['fidelity'] = pd.to_numeric(data_df['fidelity'], errors='coerce')\n",
    "                        data_df.dropna(subset=['fidelity'], inplace=True) # Drop rows where fidelity is NaN\n",
    "\n",
    "                        if data_df.empty:\n",
    "                            # print(f\"  - Warning: No valid numeric fidelity data after cleaning: {filename}\")\n",
    "                            skipped_files.append(filename)\n",
    "                            continue\n",
    "                        # --- End Check 3 --- #\n",
    "\n",
    "                        # --- Metadata Loading (keep as is) --- #\n",
    "                        metadata_df = None\n",
    "                        metadata_path = file_path.replace(\".csv\", \"_metadata.csv\")\n",
    "                        if os.path.exists(metadata_path):\n",
    "                             try:\n",
    "                                 metadata_df = pd.read_csv(metadata_path).iloc[0].to_dict()\n",
    "                             except Exception as meta_e:\n",
    "                                 print(f\"  - Warning: Could not load metadata file {metadata_path}: {meta_e}\")\n",
    "                        # --- End Metadata Loading --- #\n",
    "\n",
    "                        # --- Calculate metrics --- #\n",
    "                        avg_fidelity = data_df['fidelity'].mean()\n",
    "                        std_fidelity = data_df['fidelity'].std()\n",
    "\n",
    "                        experiment_entry = {\n",
    "                             \"initial_state\": initial_state,\n",
    "                             \"qec_method\": qec_method,\n",
    "                             \"distance\": distance,\n",
    "                             \"error_type\": error_type,\n",
    "                             \"error_params\": error_params,\n",
    "                             \"avg_fidelity\": avg_fidelity,\n",
    "                             \"std_fidelity\": std_fidelity,\n",
    "                             \"num_iterations\": len(data_df),\n",
    "                             \"metadata\": metadata_df,\n",
    "                             \"filename\": filename\n",
    "                        }\n",
    "                        if load_full_data:\n",
    "                            experiment_entry[\"data\"] = data_df\n",
    "\n",
    "                        all_experiments_data.append(experiment_entry)\n",
    "                        loaded_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        # Catch errors during pd.read_csv or subsequent checks\n",
    "                        # print(f\"  - Error processing file content for {filename}: {e}\")\n",
    "                        file_parse_errors.append((filename, str(e)))\n",
    "                        skipped_files.append(filename)\n",
    "\n",
    "                except Exception as parse_e:\n",
    "                    # print(f\"  - Error parsing parameters for {filename}: {parse_e}\")\n",
    "                    file_parse_errors.append((filename, f\"Param parsing error: {parse_e}\"))\n",
    "                    skipped_files.append(filename)\n",
    "            else:\n",
    "                skipped_files.append(filename)\n",
    "\n",
    "    print(f\"Finished loading. Loaded {loaded_count} experiment configurations.\")\n",
    "    if skipped_files:\n",
    "        print(f\"Skipped {len(skipped_files)} files (check format/content/errors).\")\n",
    "    if file_parse_errors:\n",
    "        print(f\"Encountered {len(file_parse_errors)} errors during file processing:\")\n",
    "        # --- Print specific errors --- #\n",
    "        for fname, err in file_parse_errors[:10]: # Print first 10 errors\n",
    "             print(f\"  - {fname}: {err}\")\n",
    "        if len(file_parse_errors) > 10:\n",
    "             print(\"  ... (additional errors truncated)\")\n",
    "        # --- End error printing --- #\n",
    "    return all_experiments_data\n",
    "\n",
    "# Load the data\n",
    "all_data = load_network_qec_data()\n",
    "print(f\"\\nTotal number of loaded experiment configurations: {len(all_data)}\")\n",
    "# Example: Print details of the first loaded experiment\n",
    "if all_data:\n",
    "    print(\"\\nExample loaded data point:\")\n",
    "    print(f\"  State: {all_data[0]['initial_state']}\")\n",
    "    print(f\"  QEC: {all_data[0]['qec_method']}\")\n",
    "    print(f\"  Dist: {all_data[0]['distance']}\")\n",
    "    print(f\"  Error Type: {all_data[0]['error_type']}\")\n",
    "    print(f\"  Error Params: {all_data[0]['error_params']}\")\n",
    "    # Safely check if 'data' key exists (depends on load_full_data flag)\n",
    "    if 'data' in all_data[0]:\n",
    "        print(f\"  Data points: {len(all_data[0]['data'])}\")\n",
    "    else:\n",
    "        print(f\"  Data points: (Full data not loaded)\")\n",
    "    print(f\"  Avg Fidelity: {all_data[0]['avg_fidelity']:.4f}\")\n",
    "    # print(f\"  Metadata: {all_data[0]['metadata']}\") # Can be long\n",
    "\n",
    "# --- Analysis Functions ---\n",
    "\n",
    "def aggregate_and_plot_error_types(df, plots_subdir=\"error_type_analysis\"):\n",
    "    \"\"\"\n",
    "    Analyzes and plots results grouped by error type.\n",
    "    - Overall fidelity per error type.\n",
    "    - Fidelity per error type broken down by initial state.\n",
    "    - Fidelity vs. error parameters (scatter plot) for baseline (no QEC).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Analyzing by Error Type ---\")\n",
    "    error_plots_dir = os.path.join(PLOTS_DIR, plots_subdir)\n",
    "    os.makedirs(error_plots_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Overall Fidelity by Error Type\n",
    "    print(\"  Plotting overall fidelity...\")\n",
    "    overall_metrics = df.groupby('error_type').agg(\n",
    "        avg_fidelity=('avg_fidelity', 'mean'),\n",
    "        std_fidelity=('avg_fidelity', 'std') # Std dev of the *average* fidelities per config\n",
    "    ).sort_values('avg_fidelity', ascending=False)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    # Use explicit bar plot for better color control from cycle\n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.85, len(overall_metrics)))\n",
    "    bars = ax.bar(overall_metrics.index, overall_metrics['avg_fidelity'],\n",
    "            yerr=overall_metrics['std_fidelity'], capsize=4, alpha=0.85,\n",
    "            color=colors, edgecolor='black', linewidth=0.5)\n",
    "    ax.set_title(r'Overall Average Fidelity by Error Type')\n",
    "    ax.set_ylabel(r'Average Fidelity $F$')\n",
    "    ax.set_xlabel(r'Error Type')\n",
    "\n",
    "    # Add std dev text annotations\n",
    "    for bar, std in zip(bars, overall_metrics['std_fidelity']):\n",
    "        height = bar.get_height()\n",
    "        if pd.notna(std):\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2., height + (overall_metrics['std_fidelity'].max() * 0.05 if pd.notna(overall_metrics['std_fidelity'].max()) else 0.01), # Adjust vertical position slightly based on max std dev\n",
    "                    rf'$\\pm{std:.2f}$',\n",
    "                    ha='center', va='bottom', fontsize=8, color='dimgray')\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(error_plots_dir, \"overall_fidelity_by_error_type.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Fidelity by Error Type and Initial State\n",
    "    print(\"  Plotting fidelity by error type and state...\")\n",
    "    # Use Mathtext-compatible ket notation\n",
    "    state_display_map = {'0': r'$|0\\rangle$', '1': r'$|1\\rangle$', '+': r'$|+\\rangle$', '-': r'$|-\\rangle$'}\n",
    "    state_error_metrics = df.groupby(['error_type', 'initial_state']).agg(\n",
    "        avg_fidelity=('avg_fidelity', 'mean'),\n",
    "        std_fidelity=('avg_fidelity', 'std')\n",
    "    ).unstack() # Pivot initial_state to columns\n",
    "\n",
    "    if not state_error_metrics.empty:\n",
    "        fig, ax = plt.subplots(figsize=(15, 8)) # Slightly wider figure\n",
    "        num_states = len(state_error_metrics['avg_fidelity'].columns)\n",
    "        colors = plt.cm.viridis(np.linspace(0, 0.85, num_states))\n",
    "\n",
    "        # Plot bars manually for better control over labels and annotations\n",
    "        bar_width = 0.8 / num_states\n",
    "        x = np.arange(len(state_error_metrics.index))\n",
    "\n",
    "        for i, state in enumerate(state_error_metrics['avg_fidelity'].columns):\n",
    "            means = state_error_metrics[('avg_fidelity', state)]\n",
    "            stds = state_error_metrics[('std_fidelity', state)]\n",
    "            positions = x - (num_states / 2 - i - 0.5) * bar_width\n",
    "\n",
    "            bars = ax.bar(positions, means, bar_width, yerr=stds,\n",
    "                          label=state_display_map.get(state, state), # Use LaTeX labels\n",
    "                          color=colors[i], alpha=0.85, capsize=3,\n",
    "                          edgecolor='black', linewidth=0.5)\n",
    "\n",
    "            # Add std dev text annotations for each bar group\n",
    "            for bar, std_val in zip(bars, stds):\n",
    "                height = bar.get_height()\n",
    "                y_err_val = std_val if pd.notna(std_val) else 0\n",
    "                if pd.notna(height):\n",
    "                    # Position text slightly above the bar top, aligned below the specified point\n",
    "                    y_pos = min(height + 0.01, 1.02)\n",
    "                    ax.text(bar.get_x() + bar.get_width() / 2., y_pos,\n",
    "                            rf'$\\pm{std_val:.2f}$' if pd.notna(std_val) else '', # Use raw f-string\n",
    "                            ha='center', va='bottom', fontsize=7, color='dimgray', clip_on=True)\n",
    "\n",
    "        ax.set_title(r'Average Fidelity by Error Type and Initial State')\n",
    "        ax.set_ylabel(r'Average Fidelity $F$')\n",
    "        ax.set_xlabel(r'Error Type')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(state_error_metrics.index, rotation=45, ha='right')\n",
    "\n",
    "        # Adjust legend position\n",
    "        ax.legend(title=r'Initial State', loc='best')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(error_plots_dir, \"fidelity_by_error_and_state.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # 3. Fidelity vs. Error Parameter (Scatter Plot for No QEC)\n",
    "    print(\"  Plotting fidelity vs. error parameters (Scatter - No QEC)...\")\n",
    "    unique_error_types = df['error_type'].unique()\n",
    "    df_no_qec = df[df['qec_method'] == 'none'].copy()\n",
    "\n",
    "    if df_no_qec.empty:\n",
    "         print(\"    No data found for 'qec_method = none'. Skipping parameter plots.\")\n",
    "         return # Skip parameter analysis if no baseline data\n",
    "\n",
    "    for error_type in unique_error_types:\n",
    "        error_df_no_qec = df_no_qec[df_no_qec['error_type'] == error_type]\n",
    "        if error_df_no_qec.empty:\n",
    "            continue\n",
    "\n",
    "        # Identify varying parameters within this error type subset\n",
    "        # Handle cases where error_params might be None or empty\n",
    "        valid_params = [p for p in error_df_no_qec['error_params'] if p and isinstance(p, dict)]\n",
    "        if not valid_params: continue\n",
    "        param_keys = list(valid_params[0].keys())\n",
    "\n",
    "        varying_params = defaultdict(list)\n",
    "        param_agg = []\n",
    "\n",
    "        for idx, row in error_df_no_qec.iterrows():\n",
    "            if not row['error_params'] or not isinstance(row['error_params'], dict): continue # Skip if no params\n",
    "            param_values = tuple(row['error_params'].get(k, None) for k in param_keys)\n",
    "            param_agg.append(param_values + (row['avg_fidelity'],))\n",
    "            for i, k in enumerate(param_keys):\n",
    "                 # Ensure the value is appended, even if None\n",
    "                 varying_params[k].append(param_values[i])\n",
    "\n",
    "\n",
    "        param_df = pd.DataFrame(param_agg, columns=param_keys + ['avg_fidelity'])\n",
    "\n",
    "        # Plot against each parameter if it varies\n",
    "        for param_key in param_keys:\n",
    "            # Filter out None values for this parameter before checking uniqueness/plotting\n",
    "            valid_param_values = [v for v in varying_params[param_key] if v is not None]\n",
    "            if not valid_param_values: continue # Skip if no valid values for this key\n",
    "\n",
    "            unique_values = sorted(list(set(valid_param_values)))\n",
    "\n",
    "            if len(unique_values) > 1: # Only plot if the parameter actually varies\n",
    "                # Average over other parameters if they exist\n",
    "                plot_data = param_df.groupby(param_key)['avg_fidelity'].agg(['mean', 'std']).reset_index()\n",
    "                # Filter out rows where the parameter might be NaN after grouping if necessary\n",
    "                plot_data = plot_data.dropna(subset=[param_key])\n",
    "                if plot_data.empty: continue\n",
    "\n",
    "                plt.figure()\n",
    "                # Create scatter plot for means with error bars\n",
    "                plt.errorbar(plot_data[param_key], plot_data['mean'], yerr=plot_data['std'],\n",
    "                             fmt='none', # No connecting lines\n",
    "                             capsize=5, alpha=0.6, color='grey', label='Std Dev') # Error bars\n",
    "                plt.scatter(plot_data[param_key], plot_data['mean'],\n",
    "                             marker='o', s=50, alpha=0.85, zorder=3, label='Mean Fidelity') # Scatter points for mean\n",
    "\n",
    "                # Use LaTeX for gamma if applicable\n",
    "                param_label = r'$\\gamma$' if 'gamma' in param_key else param_key.replace('_', ' ')\n",
    "                plt.title(f\"Fidelity (No QEC) vs. {param_label} for {error_type.replace('_',' ').title()}\")\n",
    "                plt.xlabel(f\"Parameter: {param_label}\")\n",
    "                plt.ylabel(r'Average Fidelity $F$')\n",
    "                plt.ylim(0, 1.05)\n",
    "                plt.grid(True, linestyle=':', alpha=0.7)\n",
    "                # Consider log scale for x-axis if parameters span orders of magnitude\n",
    "                numeric_unique_values = [v for v in unique_values if isinstance(v, (int, float))]\n",
    "                if numeric_unique_values and len(numeric_unique_values) > 1 and \\\n",
    "                   min(numeric_unique_values) > 0 and \\\n",
    "                   max(numeric_unique_values) / min(numeric_unique_values) > 100:\n",
    "                       plt.xscale('log')\n",
    "                plt.legend(loc='best')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(error_plots_dir, f\"{error_type}_vs_{param_key}_no_qec_scatter.png\"))\n",
    "                plt.close()\n",
    "\n",
    "def analyze_qec_methods(df, plots_subdir=\"qec_method_analysis\"):\n",
    "    \"\"\"\n",
    "    Analyzes and plots results grouped by QEC method.\n",
    "    - Compares QEC methods against different error types (with std dev annotations).\n",
    "    - Compares QEC methods for different initial states (with std dev annotations).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Analyzing by QEC Method ---\")\n",
    "    qec_plots_dir = os.path.join(PLOTS_DIR, plots_subdir)\n",
    "    os.makedirs(qec_plots_dir, exist_ok=True)\n",
    "\n",
    "    qec_methods = sorted([m for m in df['qec_method'].unique() if m != 'none'])\n",
    "    error_types = sorted(df['error_type'].unique())\n",
    "    initial_states = sorted(df['initial_state'].unique())\n",
    "\n",
    "    # Define readable names and consistent colors\n",
    "    method_display = {\n",
    "        'three_qubit_bit_flip': '3QB Bit', 'three_qubit_phase_flip': '3QB Phase',\n",
    "        'shor_nine': 'Shor-9', 'none': 'No QEC'\n",
    "    }\n",
    "    method_colors = plt.cm.tab10(np.linspace(0, 1, len(qec_methods) + 1))\n",
    "    method_color_map = {m: method_colors[i] for i, m in enumerate(qec_methods + ['none'])}\n",
    "\n",
    "\n",
    "    # 1. QEC Performance vs. Error Type\n",
    "    print(\"  Plotting QEC performance vs. error type...\")\n",
    "    qec_error_metrics = df.groupby(['qec_method', 'error_type']).agg(\n",
    "        avg_fidelity=('avg_fidelity', 'mean'),\n",
    "        std_fidelity=('avg_fidelity', 'std')\n",
    "    ).unstack(level=0) # Pivot qec_method to columns\n",
    "\n",
    "    if not qec_error_metrics.empty:\n",
    "        fig, ax = plt.subplots(figsize=(16, 8)) # Wider figure\n",
    "        num_methods = len(qec_error_metrics['avg_fidelity'].columns)\n",
    "        bar_width = 0.8 / num_methods\n",
    "        x = np.arange(len(qec_error_metrics.index))\n",
    "\n",
    "        for i, method in enumerate(qec_error_metrics['avg_fidelity'].columns):\n",
    "            means = qec_error_metrics[('avg_fidelity', method)]\n",
    "            stds = qec_error_metrics[('std_fidelity', method)]\n",
    "            positions = x - (num_methods / 2 - i - 0.5) * bar_width\n",
    "\n",
    "            bars = ax.bar(positions, means, bar_width, yerr=stds,\n",
    "                          label=method_display.get(method, method),\n",
    "                          color=method_color_map.get(method, 'grey'),\n",
    "                          alpha=0.85, capsize=3, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "            # Add std dev text annotations\n",
    "            for bar, std_val in zip(bars, stds):\n",
    "                height = bar.get_height()\n",
    "                y_err_val = std_val if pd.notna(std_val) else 0\n",
    "                if pd.notna(height):\n",
    "                    # Position text slightly above the bar top, aligned below the specified point\n",
    "                    y_pos = min(height + 0.01, 1.02)\n",
    "                    ax.text(bar.get_x() + bar.get_width() / 2., y_pos,\n",
    "                            rf'$\\pm{std_val:.2f}$' if pd.notna(std_val) else '', # Use raw f-string\n",
    "                            ha='center', va='bottom', fontsize=7, color='dimgray', clip_on=True)\n",
    "\n",
    "        ax.set_title(r'QEC Method Performance by Error Type')\n",
    "        ax.set_ylabel(r'Average Fidelity $F$')\n",
    "        ax.set_xlabel(r'Error Type')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(qec_error_metrics.index, rotation=45, ha='right')\n",
    "        ax.legend(title=\"QEC Method\", loc='best')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(qec_plots_dir, \"qec_vs_error_type.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # 2. QEC Performance vs. Initial State\n",
    "    print(\"  Plotting QEC performance vs. initial state...\")\n",
    "    qec_state_metrics = df.groupby(['qec_method', 'initial_state']).agg(\n",
    "        avg_fidelity=('avg_fidelity', 'mean'),\n",
    "        std_fidelity=('avg_fidelity', 'std')\n",
    "    ).unstack(level=0) # Pivot qec_method to columns\n",
    "\n",
    "    if not qec_state_metrics.empty:\n",
    "        fig, ax = plt.subplots(figsize=(13, 7))\n",
    "        num_methods = len(qec_state_metrics['avg_fidelity'].columns)\n",
    "        bar_width = 0.8 / num_methods\n",
    "        x = np.arange(len(qec_state_metrics.index))\n",
    "        # Use Mathtext-compatible ket notation (ensure consistency)\n",
    "        state_display_map = {'0': r'$|0\\rangle$', '1': r'$|1\\rangle$', '+': r'$|+\\rangle$', '-': r'$|-\\rangle$'}\n",
    "\n",
    "        for i, method in enumerate(qec_state_metrics['avg_fidelity'].columns):\n",
    "            means = qec_state_metrics[('avg_fidelity', method)]\n",
    "            stds = qec_state_metrics[('std_fidelity', method)]\n",
    "            positions = x - (num_methods / 2 - i - 0.5) * bar_width\n",
    "\n",
    "            bars = ax.bar(positions, means, bar_width, yerr=stds,\n",
    "                          label=method_display.get(method, method),\n",
    "                          color=method_color_map.get(method, 'grey'),\n",
    "                          alpha=0.85, capsize=3, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "            # Add std dev text annotations\n",
    "            for bar, std_val in zip(bars, stds):\n",
    "                height = bar.get_height()\n",
    "                y_err_val = std_val if pd.notna(std_val) else 0\n",
    "                if pd.notna(height):\n",
    "                    # Position text slightly above the bar top, aligned below the specified point\n",
    "                    y_pos = min(height + 0.01, 1.02)\n",
    "                    ax.text(bar.get_x() + bar.get_width() / 2., y_pos,\n",
    "                            rf'$\\pm{std_val:.2f}$' if pd.notna(std_val) else '', # Use raw f-string\n",
    "                            ha='center', va='bottom', fontsize=7, color='dimgray', clip_on=True)\n",
    "\n",
    "        ax.set_title(r'QEC Method Performance by Initial State')\n",
    "        ax.set_ylabel(r'Average Fidelity $F$')\n",
    "        ax.set_xlabel(r'Initial State')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([state_display_map.get(s, s) for s in qec_state_metrics.index])\n",
    "        ax.legend(title=\"QEC Method\", loc='best')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(qec_plots_dir, \"qec_vs_initial_state.png\"))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def analyze_distance(df, plots_subdir=\"distance_analysis\"):\n",
    "    \"\"\"\n",
    "    Analyzes and plots fidelity as a function of distance.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Analyzing by Distance ---\")\n",
    "    dist_plots_dir = os.path.join(PLOTS_DIR, plots_subdir)\n",
    "    os.makedirs(dist_plots_dir, exist_ok=True)\n",
    "\n",
    "    # Group by distance, error_type, and qec_method\n",
    "    dist_metrics = df.groupby(['distance', 'error_type', 'qec_method']).agg(\n",
    "        avg_fidelity=('avg_fidelity', 'mean'),\n",
    "        std_fidelity=('avg_fidelity', 'std') # Std dev of average fidelities across param variations\n",
    "    ).reset_index()\n",
    "\n",
    "    error_types = sorted(dist_metrics['error_type'].unique())\n",
    "    qec_methods = sorted(dist_metrics['qec_method'].unique())\n",
    "    method_display = {\n",
    "        'three_qubit_bit_flip': '3QB Bit', 'three_qubit_phase_flip': '3QB Phase',\n",
    "        'shor_nine': 'Shor-9', 'none': 'No QEC'\n",
    "    }\n",
    "    markers = ['o', 's', '^', 'd', 'v', 'p', '*', 'X'] # Different markers for QEC methods\n",
    "    colors = plt.cm.tab10 # Use a colormap\n",
    "\n",
    "    # Create one plot per error type, showing different QEC methods\n",
    "    for i, error_type in enumerate(error_types):\n",
    "        plt.figure()\n",
    "        ax = plt.gca()\n",
    "        subset_error = dist_metrics[dist_metrics['error_type'] == error_type]\n",
    "\n",
    "        method_colors_dist = plt.cm.viridis(np.linspace(0, 0.85, len(qec_methods)))\n",
    "        for j, qec_method in enumerate(qec_methods):\n",
    "            subset_qec = subset_error[subset_error['qec_method'] == qec_method].sort_values('distance')\n",
    "            if not subset_qec.empty:\n",
    "                ax.errorbar(subset_qec['distance'], subset_qec['avg_fidelity'], yerr=subset_qec['std_fidelity'],\n",
    "                            label=method_display.get(qec_method, qec_method),\n",
    "                            marker=markers[j % len(markers)], capsize=3, linestyle='-', linewidth=1.5, markersize=5,\n",
    "                            color=method_colors_dist[j] )\n",
    "\n",
    "        ax.set_title(f\"Fidelity vs. Distance for {error_type.replace('_', ' ').title()} Error\")\n",
    "        ax.set_xlabel(r'Distance (km)')\n",
    "        ax.set_ylabel(r'Average Fidelity $F$')\n",
    "        ax.legend(title=\"QEC Method\")\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.set_xlim(left=0)\n",
    "        ax.grid(True, linestyle=':', alpha=0.6)\n",
    "        # Use minor ticks\n",
    "        ax.xaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(dist_plots_dir, f\"fidelity_vs_distance_{error_type}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# --- Add these new plotting functions below the existing ones ---\n",
    "\n",
    "def create_distribution_fidelity_plots(all_data, plots_subdir=\"fidelity_distributions\"):\n",
    "    \"\"\"Create fidelity distribution visualizations (violin plots).\"\"\"\n",
    "    print(\"\\n--- Creating Fidelity Distribution Plots ---\")\n",
    "    if not all_data:\n",
    "        print(\"  No data available for plotting.\")\n",
    "        return\n",
    "    # Check if full data was loaded\n",
    "    if \"data\" not in all_data[0]:\n",
    "         print(\"  Full iteration data not loaded. Run load_network_qec_data with load_full_data=True.\")\n",
    "         print(\"  Skipping distribution plots.\")\n",
    "         return\n",
    "\n",
    "    violin_dir = os.path.join(PLOTS_DIR, plots_subdir)\n",
    "    os.makedirs(violin_dir, exist_ok=True)\n",
    "\n",
    "    states = sorted({exp['initial_state'] for exp in all_data})\n",
    "    state_display_map = {'0': r'$|0\\rangle$', '1': r'$|1\\rangle$', '+': r'$|+\\rangle$', '-': r'$|-\\rangle$'} # Use compatible kets\n",
    "\n",
    "    # 1. Combined violin plot of all states\n",
    "    print(\"  Creating combined violin plot of all states...\")\n",
    "    fig, ax = plt.subplots(figsize=(14, 8)) # Adjusted size\n",
    "\n",
    "    all_violins = []\n",
    "    plot_labels = []\n",
    "    positions = []\n",
    "    mean_values = {}\n",
    "\n",
    "    for i, state in enumerate(states):\n",
    "        # Concatenate fidelity data from *all* relevant experiments for this state\n",
    "        state_data = [exp['data']['fidelity'].values for exp in all_data\n",
    "                     if exp['initial_state'] == state and \"data\" in exp] # Ensure 'data' exists\n",
    "\n",
    "        if state_data:\n",
    "            concatenated_data = np.concatenate(state_data)\n",
    "            if len(concatenated_data) > 0: # Ensure we have data points\n",
    "                 all_violins.append(concatenated_data)\n",
    "                 plot_labels.append(state_display_map.get(state, state)) # Use display name\n",
    "                 positions.append(i + 1)\n",
    "                 mean_values[state] = np.mean(concatenated_data)\n",
    "\n",
    "    if all_violins:\n",
    "        parts = ax.violinplot(all_violins, positions=positions, showmeans=False, showextrema=True, widths=0.8)\n",
    "\n",
    "        # Style violins\n",
    "        colors = plt.cm.viridis(np.linspace(0, 0.9, len(plot_labels)))\n",
    "        for i, pc in enumerate(parts['bodies']):\n",
    "            pc.set_facecolor(colors[i])\n",
    "            pc.set_edgecolor('black')\n",
    "            pc.set_alpha(0.7)\n",
    "        # Style lines\n",
    "        for partname in ('cbars','cmins','cmaxes'):\n",
    "             vp = parts[partname]\n",
    "             vp.set_edgecolor('black')\n",
    "             vp.set_linewidth(1)\n",
    "\n",
    "        ax.set_title(\"Overall Fidelity Distribution Across Initial States\", pad=20, fontsize=18)\n",
    "        ax.set_ylabel(\"Fidelity\", fontsize=14)\n",
    "        ax.set_xlabel(\"Initial State\", fontsize=14)\n",
    "        ax.set_xticks(positions)\n",
    "        ax.set_xticklabels(plot_labels)\n",
    "        ax.set_ylim(-0.05, 1.05) # Adjusted ylim for visibility\n",
    "\n",
    "        ax.yaxis.set_major_locator(MultipleLocator(0.1))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "        ax.grid(True, which='major', axis='y', alpha=0.6, linestyle='-')\n",
    "        ax.grid(True, which='minor', axis='y', alpha=0.3, linestyle=':')\n",
    "\n",
    "        # Create legend with mean values\n",
    "        legend_elements = [\n",
    "            mpl.lines.Line2D([0], [0], marker='o', color='w', label=f'{plot_labels[i]}: Mean={mean_values[state]:.3f}',\n",
    "                             markerfacecolor=colors[i], markersize=8, alpha=0.8)\n",
    "            for i, state in enumerate(states) if state in mean_values\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, title=\"State Means\", loc='upper right', fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(violin_dir, \"all_states_fidelity_violin.png\"), dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"  No data found to create combined state violin plot.\")\n",
    "\n",
    "\n",
    "def create_distribution_state_error_plots(all_data, plots_subdir=\"state_error_distributions\"):\n",
    "    \"\"\"Create improved violin plots for each state showing distributions per error type\"\"\"\n",
    "    print(\"\\n--- Creating State vs Error Type Distribution Plots ---\")\n",
    "    if not all_data or \"data\" not in all_data[0]:\n",
    "         print(\"  Full iteration data not loaded or no data. Skipping distribution plots.\")\n",
    "         return\n",
    "\n",
    "    plot_dir = os.path.join(PLOTS_DIR, plots_subdir)\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "    states = sorted({exp['initial_state'] for exp in all_data})\n",
    "    error_types = sorted({exp['error_type'] for exp in all_data})\n",
    "\n",
    "    for state in states:\n",
    "        # Determine grid size\n",
    "        n_errors = len(error_types)\n",
    "        n_cols = 3\n",
    "        n_rows = (n_errors + n_cols - 1) // n_cols\n",
    "        fig, axs = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows), squeeze=False)\n",
    "        fig.suptitle(f\"Fidelity Distribution by Error Type: State {state}\", fontsize=18, y=1.0) # Adjusted y\n",
    "\n",
    "        axs_flat = axs.flatten()\n",
    "        plot_idx = 0\n",
    "        for error_type in error_types:\n",
    "            ax = axs_flat[plot_idx]\n",
    "\n",
    "            # Aggregate all fidelity data for this state and error type across all params/QEC\n",
    "            state_error_data = [exp['data']['fidelity'].values for exp in all_data\n",
    "                                if exp['initial_state'] == state and exp['error_type'] == error_type and \"data\" in exp]\n",
    "\n",
    "            if state_error_data:\n",
    "                concatenated_data = np.concatenate(state_error_data)\n",
    "                if len(concatenated_data) > 0:\n",
    "                    parts = ax.violinplot(concatenated_data, showmeans=False, showextrema=True, widths=0.8)\n",
    "                    # Style violin\n",
    "                    for pc in parts['bodies']:\n",
    "                        pc.set_facecolor(plt.cm.tab10(plot_idx % 10)) # Use consistent color cycle\n",
    "                        pc.set_edgecolor('black')\n",
    "                        pc.set_alpha(0.7)\n",
    "                    for partname in ('cbars','cmins','cmaxes'):\n",
    "                        vp = parts[partname]\n",
    "                        vp.set_edgecolor('black')\n",
    "                        vp.set_linewidth(1)\n",
    "\n",
    "                    # Add mean value text annotation in the top right of the subplot\n",
    "                    mean_val = np.mean(concatenated_data)\n",
    "                    ax.text(0.95, 0.95, f'Mean: {mean_val:.3f}',\n",
    "                            transform=ax.transAxes, # Use axes coordinates\n",
    "                            ha='right', va='top', fontsize=10,\n",
    "                            bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.7, ec='grey'))\n",
    "\n",
    "                    ax.set_ylim(-0.05, 1.05)\n",
    "                    ax.set_xticks([]) # No x-ticks needed for single violin\n",
    "                    ax.set_ylabel(\"Fidelity\")\n",
    "                    ax.set_title(error_type.replace('_', ' ').title(), fontsize=14)\n",
    "                    ax.yaxis.set_major_locator(MultipleLocator(0.2))\n",
    "                    ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "                    ax.grid(True, which='both', axis='y', alpha=0.5, linestyle=':')\n",
    "                    plot_idx += 1\n",
    "                else:\n",
    "                     ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes)\n",
    "                     ax.set_title(error_type.replace('_', ' ').title(), fontsize=14)\n",
    "                     ax.set_xticks([])\n",
    "                     ax.set_yticks([])\n",
    "                     plot_idx += 1\n",
    "            else:\n",
    "                 ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes)\n",
    "                 ax.set_title(error_type.replace('_', ' ').title(), fontsize=14)\n",
    "                 ax.set_xticks([])\n",
    "                 ax.set_yticks([])\n",
    "                 plot_idx += 1\n",
    "\n",
    "        # Hide unused subplots\n",
    "        for i in range(plot_idx, len(axs_flat)):\n",
    "            axs_flat[i].set_visible(False)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # Adjust layout rect for suptitle\n",
    "        plt.savefig(os.path.join(plot_dir, f\"{state}_error_type_distribution.png\"), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def create_distribution_qec_plots(all_data, plots_subdir=\"qec_method_distributions\"):\n",
    "    \"\"\"Create violin/box plots comparing QEC method fidelity distributions.\"\"\"\n",
    "    print(\"\\n--- Creating QEC Method Distribution Plots ---\")\n",
    "    if not all_data or \"data\" not in all_data[0]:\n",
    "         print(\"  Full iteration data not loaded or no data. Skipping distribution plots.\")\n",
    "         return\n",
    "\n",
    "    plot_dir = os.path.join(PLOTS_DIR, plots_subdir)\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "    states = sorted({exp['initial_state'] for exp in all_data})\n",
    "    qec_methods = sorted({exp['qec_method'] for exp in all_data}) # Include 'none'\n",
    "    state_display_map = {'0': r'$|0\\rangle$', '1': r'$|1\\rangle$', '+': r'$|+angle$', '-': r'$|-angle$'} # For title\n",
    "\n",
    "    method_display = { # Define readable names\n",
    "        'three_qubit_bit_flip': '3QB Bit', 'three_qubit_phase_flip': '3QB Phase',\n",
    "        'shor_nine': 'Shor-9', 'none': 'No QEC'\n",
    "    }\n",
    "    method_colors = plt.cm.viridis(np.linspace(0, 0.9, len(qec_methods)))\n",
    "    method_color_map = {m: method_colors[i] for i, m in enumerate(qec_methods)}\n",
    "\n",
    "    for state in states:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        ax = plt.gca()\n",
    "\n",
    "        plot_data = []\n",
    "        plot_labels = []\n",
    "        plot_positions = []\n",
    "        mean_texts = []\n",
    "        current_pos = 1\n",
    "\n",
    "        for method in qec_methods: # Iterate through all methods including 'none'\n",
    "            # Aggregate all fidelity data for this state and QEC method\n",
    "            method_state_data = [exp['data']['fidelity'].values for exp in all_data\n",
    "                                 if exp['initial_state'] == state and exp['qec_method'] == method and \"data\" in exp]\n",
    "\n",
    "            if method_state_data:\n",
    "                concatenated_data = np.concatenate(method_state_data)\n",
    "                if len(concatenated_data) > 0:\n",
    "                    plot_data.append(concatenated_data)\n",
    "                    plot_labels.append(method_display.get(method, method))\n",
    "                    plot_positions.append(current_pos)\n",
    "                    mean_texts.append(f'{method_display.get(method, method)}: {np.mean(concatenated_data):.3f}')\n",
    "                    current_pos += 1\n",
    "\n",
    "        if plot_data:\n",
    "            # Violin Plot\n",
    "            vparts = ax.violinplot(plot_data, positions=plot_positions, showmeans=False, showextrema=False, widths=0.7)\n",
    "            for i, pc in enumerate(vparts['bodies']):\n",
    "                pc.set_facecolor(method_color_map.get(qec_methods[i], 'grey')) # Use original method name for color lookup\n",
    "                pc.set_edgecolor('black')\n",
    "                pc.set_alpha(0.4)\n",
    "\n",
    "            # Box Plot overlay\n",
    "            bparts = ax.boxplot(plot_data, positions=plot_positions, showfliers=False, patch_artist=True, widths=0.3,\n",
    "                                medianprops=dict(color='black', linewidth=1.5))\n",
    "            for i, box in enumerate(bparts['boxes']):\n",
    "                box.set_facecolor(method_color_map.get(qec_methods[i], 'grey'))\n",
    "                box.set_alpha(0.8)\n",
    "                box.set_edgecolor('black')\n",
    "\n",
    "            ax.set_title(f\"Fidelity Distribution by QEC Method for State {state_display_map.get(state, state)}\", pad=20)\n",
    "            ax.set_ylabel(\"Fidelity\")\n",
    "            ax.set_xlabel(\"QEC Method\")\n",
    "            ax.set_xticks(plot_positions)\n",
    "            ax.set_xticklabels(plot_labels, rotation=30, ha='right')\n",
    "            ax.set_ylim(-0.05, 1.05)\n",
    "            ax.yaxis.set_major_locator(MultipleLocator(0.1))\n",
    "            ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "            ax.grid(True, which='both', axis='y', alpha=0.5, linestyle=':')\n",
    "\n",
    "            # Add text box with all means in the top right\n",
    "            mean_summary_text = 'Means:\\n' + '\\n'.join(mean_texts)\n",
    "            ax.text(0.97, 0.97, mean_summary_text,\n",
    "                    transform=ax.transAxes, ha='right', va='top',\n",
    "                    fontsize=9, bbox=dict(boxstyle='round,pad=0.5', fc='white', alpha=0.8))\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plot_dir, f\"{state}_qec_method_distribution.png\"), dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(f\"  No data found to create QEC distribution plot for state {state}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f848166-b52f-472f-93d7-13c943d37d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading experiment data from: results_network_qec_v2 (Load full data: True)\n",
      "Finished loading. Loaded 380 experiment configurations.\n",
      "Original number of configurations loaded: 380\n",
      "Number of configurations after filtering 'fibre_loss': 379\n",
      "\n",
      "--- Analyzing by Error Type ---\n",
      "  Plotting overall fidelity...\n",
      "  Plotting fidelity by error type and state...\n",
      "  Plotting fidelity vs. error parameters (Scatter - No QEC)...\n",
      "\n",
      "--- Analyzing by QEC Method ---\n",
      "  Plotting QEC performance vs. error type...\n",
      "  Plotting QEC performance vs. initial state...\n",
      "\n",
      "--- Analyzing by Distance ---\n",
      "\n",
      "--- Creating Fidelity Distribution Plots ---\n",
      "  Creating combined violin plot of all states...\n",
      "\n",
      "--- Creating State vs Error Type Distribution Plots ---\n",
      "\n",
      "--- Creating QEC Method Distribution Plots ---\n",
      "\n",
      "Analysis complete. Plots saved to: plots_network_qec_v2\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    configure_plots()\n",
    "    # Load data *with* full iteration details for distribution plots\n",
    "    all_data = load_network_qec_data(load_full_data=True)\n",
    "\n",
    "    if all_data:\n",
    "        # Create a DataFrame for easier analysis of *averages*\n",
    "        df_all_metrics = pd.DataFrame([{k: v for k, v in exp.items() if k != 'data'} for exp in all_data])\n",
    "        print(f\"Original number of configurations loaded: {len(df_all_metrics)}\")\n",
    "\n",
    "        # --- Filter out 'fibre_loss' error type --- #\n",
    "        df_filtered_metrics = df_all_metrics[df_all_metrics['error_type'] != 'fibre_loss'].copy()\n",
    "        all_data_filtered = [exp for exp in all_data if exp['error_type'] != 'fibre_loss']\n",
    "        print(f\"Number of configurations after filtering 'fibre_loss': {len(df_filtered_metrics)}\")\n",
    "        # ------------------------------------------ #\n",
    "\n",
    "        # Run the analyses based on average metrics\n",
    "        aggregate_and_plot_error_types(df_filtered_metrics)\n",
    "        analyze_qec_methods(df_filtered_metrics)\n",
    "        analyze_distance(df_filtered_metrics)\n",
    "\n",
    "        # Run the analyses based on full distributions\n",
    "        create_distribution_fidelity_plots(all_data_filtered)\n",
    "        create_distribution_state_error_plots(all_data_filtered)\n",
    "        create_distribution_qec_plots(all_data_filtered)\n",
    "\n",
    "\n",
    "        print(f\"\\nAnalysis complete. Plots saved to: {PLOTS_DIR}\")\n",
    "    else:\n",
    "        print(\"No data loaded, cannot perform analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0c800-cf0b-4f56-a594-91cd17d19f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
